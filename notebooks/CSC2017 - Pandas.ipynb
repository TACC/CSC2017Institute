{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Stack Exchance\n",
    "\n",
    "While everyone *loves* a fun dataset to explore, good data is expensive. It costs a significant amount of resources to generate, accurately curate, securely store, and provide robust access to. For instance, our *relatively* cold-storage tape archive, [Ranch](https://www.tacc.utexas.edu/systems/ranch), grows at a rate of 8.5PB (~5.3%) per year. Despite the cost, data is often invaluable to both users and administrators.\n",
    "\n",
    "Everyone that is attending this camp has probaby interacted with at least one Stack Exchange Community:\n",
    "\n",
    "[**All Sites**](https://stackexchange.com/sites)\n",
    "- StackOverflow\n",
    "- Super User\n",
    "- TeX - LaTeX\n",
    "- ...and more\n",
    "\n",
    "An understanding of where knowledge is lacking in your scientific or technological community will allow you to improve your contributions as a whole.\n",
    "\n",
    "In this section, we will be utilizing our python skills to explore question and answer history from [Stack Exchange](https://stackexchange.com/) that is made available over a RESTful interface. We will be querying and analyzing their **actual** data, and these methods can be extended to a variety of other datasets and websites.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Use python requests to download data\n",
    "- Import data into [Pandas](http://pandas.pydata.org/)\n",
    "- Explore data\n",
    "  - Inspect and summarize data\n",
    "  - Group records\n",
    "  - Select and subset records\n",
    "  - Visualize selection\n",
    "  - Join two datasets together\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- [**requests** library](http://docs.python-requests.org/en/master/) *\\(Already Installed\\)*\n",
    "- [**pandas** library](http://pandas.pydata.org/) *\\(Already Installed\\)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Exchange Questions\n",
    "\n",
    "Stack Exchange has a well documented [API](https://api.stackexchange.com/), which contains enpoints for **each site**. While the API allowed for any interaction while authenticated, general information can be retrieved without creating an account.\n",
    "\n",
    "Beginning with the initial questsions submitted by users, take a look at the\n",
    "\n",
    "[Questions API](https://api.stackexchange.com/docs/questions)\n",
    "\n",
    "webapp on the main site, and build a query that you would like to use with Python.\n",
    "\n",
    "## Goals\n",
    "- Choose a site (default is StackOverflow)\n",
    "- Choose Start and/or End Date\n",
    "- Sort by creation\n",
    "- Limit the number of questions to 1000 (`pagesize`)\n",
    "\n",
    "# Transfer to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.stackexchange.com/2.2/questions'\n",
    "\n",
    "params = dict(\n",
    "    site='stackoverflow', # stackoverflow (coding) questions\n",
    "    pagesize='10',        # Number of questions to return\n",
    "    fromdate='1500163200',#Get epoch time from website\n",
    "    order='desc',\n",
    "    sort='creation'\n",
    ")\n",
    "\n",
    "import json\n",
    "resp = requests.get(url=url, params=params)\n",
    "data = json.loads(resp.text)\n",
    "\n",
    "print(json.dumps(data, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! If you kept `pagesize` at 10, you should have a JSON response of 10 questsions. If you decided to crank up your response size, you might have to scroll a bit.\n",
    "\n",
    "## JSON Structure\n",
    "\n",
    "This JSON response probably looks familiar if you have ever worked with Python dictionaries in the past. At the most basic level, a JSON is a collection of key, value pairs.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"key1\": value1,\n",
    "    \"key2\": value2\n",
    "}\n",
    "```\n",
    "\n",
    "Instead of using a numerical index, you refer to each value with the corresponding key. While this makes the data structure human-readable and single-value retrieval and convenient, the nested nature makes makes traditional iteration and indices somewhat difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first question title\n",
    "############################\n",
    "# Pull \"items\" json\n",
    "#  > Pull first record\n",
    "#    > Pull title\n",
    "print(data['items'][0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to know the exact key names to traverse it\n",
    "for item in data['items']:\n",
    "    print(\"TAGS - %s\"%(item['title']))\n",
    "    print(\"   [%s]\\n\"%(\", \".join(item['tags'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "- Try pulling out the `answer_count` for each question\n",
    "- Try pulling out the `view_count` for each question\n",
    "- Try pulling out the submission date.\n",
    "- **Extra Credit** - [Convert the epoch time to human readable](https://stackoverflow.com/a/12400584)\n",
    "\n",
    "# Converting to Pandas\n",
    "\n",
    "Instead of testing you on your ability to traverse a JSON tree, the goal for today is to explore data using Pandas, so lets convert the JSON to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questionsDF = pd.io.json.json_normalize(data['items'])\n",
    "questionsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`json_normalize`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html) takes a nested JSON and flattens it into a table. In our case, it flattened each return question in the `items` list. Child JSONs like owner, which described the original submitter, now have owner as a prefix in the column name.\n",
    "\n",
    "### JSON\n",
    "```\n",
    "\"owner\": {\n",
    "            \"reputation\": 1,\n",
    "            \"user_id\": 6140730,\n",
    "            \"user_type\": \"registered\",\n",
    "            \"profile_image\": \"https://www.gravatar.com/avatar/efa02138df0bc1f59618c365872caed6?s=128&d=identicon&r=PG&f=1\",\n",
    "            \"display_name\": \"John\",\n",
    "            \"link\": \"https://stackoverflow.com/users/6140730/john\"\n",
    "         }\n",
    "```\n",
    "### Table\n",
    "\n",
    "| Column Name | Value |\n",
    "|--|--|\n",
    "| owner.reputation | 1 |\n",
    "| owner.user_id | 6140730 |\n",
    "| owner.user_type | registered |\n",
    "| owner.profile_image | https://www... |\n",
    "| owner.display_name | John |\n",
    "| owner.link | https://stackoverflow... |\n",
    "\n",
    "# Exploring the Data\n",
    "\n",
    "When we transform the JSON data into a table, using `json_normalize`, the resulting table is actually a [Pandas DataFrame.](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)\n",
    "\n",
    "A DataFrame is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, factors and more) in columns. It is similar to a spreadsheet or an SQL table or the data.frame in R. A DataFrame always has an index (0-based). An index refers to the position of an element in the data structure.\n",
    "\n",
    "You can see the **bold** index column on the left of our example.\n",
    "\n",
    "## Viewing DataFrame Attributes\n",
    "\n",
    "Besides having column headers, DataFrames come with some nice attributes and methods to view specific parts of the data.\n",
    "\n",
    "## Columns\n",
    "\n",
    "You often need to iterate over the columns of your table, and DataFrames expose those names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questionsDF.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape\n",
    "\n",
    "You can also see how many rows and columns (rows, columns) are in your DataFrame by accessing the shape attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questionsDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head\n",
    "\n",
    "If you have ever used the `head` command on a terminal to view the first N lines of a file, the head function of a DataFrame will look familiar to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF.head()\n",
    "#questionsDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tail\n",
    "\n",
    "There is also a tail command for looking at the last N rows of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF.tail()\n",
    "#questionsDF.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Records\n",
    "\n",
    "You may not actually want to use each column in this table, so you can generate summary statistics grouped by subsets or attributes. For example, we could be interested in looked at the `view_count` of each question, and see how many people might also have this problem.\n",
    "\n",
    "## Column Groups\n",
    "\n",
    "You can pull out the data for `view_count` using the syntax\n",
    "\n",
    "```\n",
    "DF[\"column name\"]\n",
    "```\n",
    "\n",
    "We can also describe column numerically using the `describe()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF['view_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides only using the first 10 questions in my example data, they're all very new, so they have very few views. Lets instead work on the latest 1,000 questions and generate the same description. Because they limit pagesize of the response, we will be pulling the first 100 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest 1000 questions\n",
    "\n",
    "# Params pull 100 questsions per query\n",
    "params = dict(\n",
    "    site='stackoverflow',\n",
    "    pagesize='100',\n",
    "    page='1',\n",
    "    order='desc',\n",
    "    sort='creation'\n",
    ")\n",
    "\n",
    "nPages = 10 #How many pages you want\n",
    "data = []\n",
    "\n",
    "import sys\n",
    "print(\"Reading Page:\")\n",
    "for page in map(str, range(1,nPages+1)):\n",
    "    params['page']=page # Change page number\n",
    "    if int(page) > 1: sys.stdout.write(\", \")\n",
    "    sys.stdout.write(\"%s\"%(page))\n",
    "    data += json.loads(requests.get(url=url, params=params).text)['items']\n",
    "\n",
    "questionsDF = pd.io.json.json_normalize(data)\n",
    "# Drop the \"migrated_from\" columns\n",
    "questionsDF = questionsDF[list(filter(lambda x: 'migrated' not in x, questionsDF.columns))]\n",
    "\n",
    "questionsDF['view_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a larger pool of data, you should check out other statistics that can be generated per column. Feel free to use another numerical column as well.\n",
    "\n",
    "## Explore\n",
    "\n",
    "There are a bunch of [built in](https://pandas.pydata.org/pandas-docs/stable/api.html#computations-descriptive-stats) descriptive functions, but these are good to check out.\n",
    "\n",
    "- describe()\n",
    "- nuniqe()\n",
    "- value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique users?\n",
    "questionsDF['owner.user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Way Groups\n",
    "\n",
    "If you ever want to summarize by one or more variables, you can use the `groupby` method. In our case, it would be interesting to look at `view_count` statistics of answered and unanswered questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF.groupby('is_answered')['view_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that while there are fewer answered questions, their view count is almost 100% higher. Neat!\n",
    "\n",
    "## Explore\n",
    "\n",
    "Take some time using the `groupby` method to explore other cool trends.\n",
    "\n",
    "- Owner reputation\n",
    "- Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questionsDF.groupby('is_answered')['owner.reputation'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting and Subsetting Records\n",
    "\n",
    "You can also select a subset of the data using criteria. For example, we can select all rows that have a `view_count` greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF[questionsDF.view_count > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "Experiment with the\n",
    "\n",
    "- `>`, `<`\n",
    "- `==`, `!=`\n",
    "- `>=`, `<=`\n",
    "\n",
    "operators on numerical data. If you have extra time, look for questions that contain tags that you know. The tags are actually a list, so you can search for tags using the `in` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF[ questionsDF.tags.map(lambda x: 'python' in x) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Data\n",
    "\n",
    "While the tables we have been generating are nice, they still contain thousands of rows. A single figure could help visualize the data as a whole. Insead of crafting specific matplotlib calls, Pandas built a universal `plot()` function into the DataFrame object to make it even easier to generate figures. You can plot single columns as histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF['view_count'].plot(kind='hist')\n",
    "# Try increasing the resolution with the \"bins\" parameter\n",
    "# Try a square root transform of the view count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot the two way tables we previously made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF.groupby('is_answered')['view_count'].plot(kind='hist', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "Try generating a few figures on your own.\n",
    "\n",
    "# Joining Tables\n",
    "\n",
    "You can even join two datasets. Lets grab some answers so we can try joining them to their corresponding questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.stackexchange.com/2.2/answers'\n",
    "params = dict(\n",
    "    site='stackoverflow',\n",
    "    pagesize='100',\n",
    "    page='1',\n",
    "    order='desc',\n",
    "    sort='creation'\n",
    ")\n",
    "\n",
    "nPages = 10 #How many pages you want\n",
    "data = []\n",
    "\n",
    "import sys\n",
    "print(\"Reading Page:\")\n",
    "for page in map(str, range(1,nPages+1)):\n",
    "    params['page']=page # Change page number\n",
    "    if int(page) > 1: sys.stdout.write(\", \")\n",
    "    sys.stdout.write(\"%s\"%(page))\n",
    "    data += json.loads(requests.get(url=url, params=params).text)['items']\n",
    "\n",
    "answersDF = pd.io.json.json_normalize(data)\n",
    "answersDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join\n",
    "\n",
    "The inner of join of S1 and S2\n",
    "```\n",
    "S1 - {A1, B1, B2, C1}\n",
    "S2 - {D1, B1, B2, C1}\n",
    "```\n",
    "would equal\n",
    "\n",
    "```\n",
    "S1B1 - S2B1\n",
    "S1B1 - S2B2\n",
    "S1B2 - S2B1\n",
    "S1B2 - S2B2\n",
    "S1C1 - S2C1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(left=questionsDF, right=answersDF[['answer_id','question_id']], left_on=\"question_id\", right_on=\"question_id\")\n",
    "print(merged.shape)\n",
    "merged.head()\n",
    "print(questionsDF.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Join\n",
    "\n",
    "Left joins return all items from the first set, and any items from the second set that overlap with the first. This is useful if we want ALL questions returned, and any questions that also match.\n",
    "\n",
    "Using the table from the first example, a left join would yield\n",
    "\n",
    "```\n",
    "S1A1 - NA\n",
    "S1B1 - S2B2\n",
    "S1B2 - S2B1\n",
    "S1B2 - S2B2\n",
    "S1C1 - S2C1\n",
    "```\n",
    "\n",
    "Notice that whenever there is no match on the right, fields are filled in as NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(left=questionsDF, right=answersDF, left_on=\"question_id\", right_on=\"question_id\", how=\"left\")\n",
    "print(merged.shape)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "There are also Right and Outer joins to explore. Take a look at [the documentation](https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging) and see if you can discover anythign fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try joining some data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
